---
title: 'OpenAI Gym environment for Franka Emika Panda robot'
date: 2021-02-13
permalink: /posts/2021/02/openai-environment-for-franka-emika-panda-robot/
tags:
  - PyBullet
  - OpenAI Gym
  - Robotics
  - Reinforcement Learning
---

![](https://raw.githubusercontent.com/qgallouedec/panda-gym/master/docs/demo.gif)

Learning is cool
======

It is no longer necessary to explain the genesis of reinforcement learning and the great applications it has. There are many school cases and the learning algorithms are more and more interesting.
I find them particularly fascinating when they can be explained with very simple ideas. 

One of the basic environments proposed by OpenAI to train reinforcement learning algorithms are the environments of the [Fetch robot](https://openai.com/blog/ingredients-for-robotics-research/).

This Fetch environments are an excellent starting point to test RL algorithms.
But, you may have noticed that these environments are based on [MuJoCo](http://www.mujoco.org): a very powerful physics engine, but not free.

In this work, I tried to reproduce as faithfully as possible these environments based on a **free** and **open-source** physics engine: [Bullet](https://pybullet.org/wordpress/).
I also chose to use another robot arm, with more joints, and a different gripper: the **Franka Emika Panda robot**.
Source code is available on [GitHub](https://github.com/qgallouedec/panda-gym). 
In the following, I will present you the result of this work, and show you some examples of use.

Installation and usage
======

Using `pip` you can install `panda-gym`by running

```bash
pip install panda-gym
```

Then, in a python script:

```python
import gym
import panda_gym

env = gym.make('PandaReach-v0', render=True)

obs = env.reset()
done = False
while not done:
    action = env.action_space.sample() # random action
    obs, reward, done, info = env.step(action)

env.close()
```

For those of you who are used to the OpenAI gym environments, you will notice that the rendering functionality is not handled in the usual way. It is passed as an argument to the `make` function. So you don't need to call the `render()` function.


Train `panda_gym` with Hindsight Experience Replay
======

[Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (HER) was introduced in 2017 by Andrychowicz et al.. The key idea of HER is to see a fail as a success, but with another goal.
It is a method that has shown very promising results in robotic environments. Since the space of action and the space of observation are identical, the results should be very similar.

<div class="infogram-embed" data-id="904c3193-f551-4da8-9c37-36ff4e47fe12" data-type="interactive" data-title="Courbes avec marge d&amp;#39;erreur">
</div>
<script>
!function(e,i,n,s){
  var t="InfogramEmbeds",d=e.getElementsByTagName("script")[0];
  if(window[t]&&window[t].initialized)window[t].process&&window[t].process();
  else if(!e.getElementById(n)){
    var o=e.createElement("script");o.async=1,o.id=n,o.src="https://e.infogram.com/js/dist/embed-loader-min.js",d.parentNode.insertBefore(o,d)
    }
  }(document,0,"infogram-async");
</script>


